[[parallel-reading]]
= Параллельное чтение данных в СУБД Firebird
Симонов Денис, Влад Хорсун
v1.0 от 03.12.2023
:doctype: book
:sectnums!:
:sectanchors:
:experimental:
:lang: ru
:imagesdir: images
ifdef::backend-pdf[]
:pdf-fontsdir: theme/fonts
:pdf-themesdir: theme/firebird-pdf
:pdf-theme: firebird
:source-highlighter: coderay
endif::[]
ifdef::backend-html5[]
:stylesdir: theme/firebird-html
:stylesheet: firebird.css
:source-highlighter: highlight.js
endif::[]

[dedication%notitle]
--
Этот материал был создан при поддержке и спонсорстве компании https://www.ibase.ru[iBase.ru], которая разрабатывает  инструменты Firebird SQL для предприятий и предоставляет сервис технической поддержки для Firebird SQL.

Материал выпущен под лицензией Public Documentation License https://www.firebirdsql.org/file/documentation/html/en/licenses/pdl/public-documentation-license.html
--

[preface]
== Предисловие

В Firebird 5.0 появилась возможность использовать параллелизм при создании резервной копии с помощью утилиты `gbak`. 
Надо отметить, что первоначально эта функция появилась в HQbird 2.5, и уже после обкатке её реальными заказчиками была портирована в Firebird 5.0.

В этой статье мы познакомимся с некоторыми механизмами, которые используется при параллельном создании резервных копий внутри утилиты `gbak`,
а также покажем как их можно использовать в ваших приложениях для параллельного чтения данных.

Важно отметить, что здесь идёт речь не о параллельном сканировании таблиц внутри движка Firebird при выполнении SQL запросов, а чтении данных внутри вашего приложения
параллельными потоками.

Для демонстрации параллельного чтения данных из СУБД Firebird я написал простенькую утилиту, которые экспортирует данные из одной или нескольких таблиц в формат CSV.
Её описание и сходный код можно найти по адресу https://github.com/IBSurgeon/FBCSVExport.git

== Организация параллельного чтения

И так, давайте подумаем как читать данные из нескольких таблиц параллельно. Для начала надо отметить, что Firebird, позволяет выполнять запросы параллельно только если каждый из запросов будет выполняться в отдельном соединении. 

Для этой цели необходимо сделать пул рабочих потоков. Основной поток приложения
также является рабочим потоком, поэтому количество дополнительных рабочих потоков должно быть равно N - 1, где N - количество параллельных обработчиков. Каждый рабочий поток будет работать с собственным соединением и транзакцией.

И тут возникает первая проблема, а как обеспечить согласованность прочитанных данных? 

=== Согласованное чтение данных

Поскольку каждый рабочий поток использует собственное соединение и собственную транзакцию, то возникает проблема несогласованного чтения - если таблица параллельно меняется, то прочитанные данные могут быть несогласованными. В однопоточном режиме `gbak` использовал транзакцию с режимом изолированности SNAPSHOT, что позволяло читать согласованную информацию, на начало старта SNAPSHOT транзакции. Но здесь у нас несколько транзакций и необходимо, чтобы они видели один и тот же "снимок", для того чтобы они читали одни и те же неизменные данные.

Механизм создания общего снимка для разных транзакций с режимом изолированности SNAPSHOT появился в Firebird 4.0 (первоначально HQBird 2.5, но в Firebird 4.0/HQBird 4.0 он более простая и эффективная). Для создания общего снимка существует два способа:

1. Через SQL.
  - получаем номер моментального снимка из главной транзакции, то есть транзакции с которой работает основной рабочий поток.
+  
[source,sql]
----
SELECT RDB$GET_CONTEXT('SYSTEM', 'SNAPSHOT_NUMBER') FROM RDB$DATABASE
----
  
  - стартуем другие транзакции запросом:
+
[source,sql]
----
SET TRANSACTION SNAPSHOT AT NUMBER snapshot_number
----
+
   где `snapshot_number` номер моментального снимка.

2. Через API.
  - получить номер моментального снимка из главной транзакции, то есть транзакции с которой работает основной рабочий поток, используя функцию
   `isc_transaction_info` или `ITransaction.getInfo` с тегом `fb_info_tra_snapshot_number`;
  - стартуем другие транзакции, передавая туда номер снимка с помощью тега `isc_tpb_at_snapshot_number`.
  
`FBCSVExport` так же как и `gbak` использует второй подход. Эти подходы можно комбинировать, например получив номер моментального снимка запросом, но передавая этот номер
для старта других транзакций с помощью API или наоборот.

В `FBCSVExport` получение номера моментального снимка реализовано следующим образом:

[source,cpp]
----
ISC_INT64 getSnapshotNumber(Firebird::ThrowStatusWrapper* status, Firebird::ITransaction* tra)
{
    ISC_INT64 ret = 0;
    unsigned char in_buf[] = { fb_info_tra_snapshot_number, isc_info_end };
    unsigned char out_buf[16] = { 0 };

    tra->getInfo(status, sizeof(in_buf), in_buf, sizeof(out_buf), out_buf);

    unsigned char* p = out_buf, * e = out_buf + sizeof(out_buf);
    while (p < e)
    {
        short len = 0;
        switch (*p++)
        {
        case isc_info_error:
        case isc_info_end:
            p = e;
            break;

        case fb_info_tra_snapshot_number:
            len = static_cast<short>(isc_vax_integer(reinterpret_cast<char*>(p), 2));
            p += 2;
            ret = isc_portable_integer(p, len);
            p += len;
            break;
        }
    }
    return ret;
}
----  

А старт транзакции с полученным номером моментального снимка так:

[source,sql]
----
Firebird::AutoDispose<Firebird::IXpbBuilder> tpbWorkerBuilder(fbUtil->getXpbBuilder(&status, Firebird::IXpbBuilder::TPB, nullptr, 0));
tpbWorkerBuilder->insertTag(&status, isc_tpb_concurrency);
tpbWorkerBuilder->insertBigInt(&status, isc_tpb_at_snapshot_number, snapshotNumber);

Firebird::AutoRelease<Firebird::ITransaction> workerTra(
    workerAtt->startTransaction(
        &status,
        tpbWorkerBuilder->getBufferLength(&status),
        tpbWorkerBuilder->getBuffer(&status)
    )
);
----

Теперь данные прочитанные из разных соединений будут согласованными, можно
распределять нагрузку по рабочим потокам.

Как именно распределить нагрузку между рабочими потоками? В случае полного экспорта таблиц или резервной копии самым простым вариантом
является - один рабочий поток на одну таблицу. Но при таком подходе есть одна проблема: а что если у нас много маленьких и одна большая таблица или вообще таблица всего одна и она огромная?
В этом случае какому-то потоку достанется большая таблица, а остальные потоки будут простаивать. Для того чтобы этого не происходило необходимо обрабатывать большую таблицу по частям.

[NOTE]
====
Дальнейшие рассуждения касаются только полного чтения таблиц, если вы хотите организовать параллельное чтение из некоторого запроса (представления),
то вам необходимо продумать как разбивать этот запрос на части самостоятельно. 
====

=== Разбиение большой таблицы на части

Допустим у нас всего одна большая таблица, которую хотим прочитать целиком и как можно быстрее.
Предлагается разбить её на несколько частей и каждую часть читать из своего
потока независимо. Каждый поток должен иметь свой коннект с БД.

В этом случае возникают следующие вопросы:

- на сколько частей разбить таблицу?
- как это лучше сделать?

Ответим на эти вопросы по порядку.

==== На сколько частей разбить таблицу

Для начала предположим идеальный вариант -- сервер и клиент больше ничем не заняты,
то есть все CPU полностью в нашем распоряжении. Тогда я бы рекомендовал:

а) взять за максимальное количество частей двойное количество ядер на сервере.
Обычно рекомендуется разбивать подобные параллельные задачи согласно количеству ядер,
но мы точно знаем, что у нас будут задержки связанные с IO, поэтому можем себе
позволить некоторое превышение. Более точно определить может только практика.

б) учитывать количество ядер на клиенте: если на сервере их сильно больше (обычная ситуация),
то возможно будет иметь смысл сильнее ограничить количество частей разбиения, чтобы не
перегружать клиента (он всё равно больше не сможет обработать, а расходы на переключение
потоков никуда не денутся). Точнее можно будет решить, наблюдая за загрузкой CPU клиента и
сервера -- если на клиенте 100%, а на сервере заметно меньше, то имеет смысл уменьшить
количество частей.

в) если клиент и сервер -- один и тот же хост, то см. (а)

Если клиент и/или сервер заняты чем-то ещё, то возможно придётся ещё уменьшить
количество частей. Так же на это может повлиять способность дисков на сервере обрабатывать
множество IO запросов одновременно (наблюдать за размером очереди и временем отклика).

==== Как лучше разбить таблицу на части

Для любой параллельной обработки важно обеспечить равномерное распределение заданий по
обработчикам и свести к минимуму их взаимную синхронизацию. Причём нужно помнить, что
синхронизация обработчиков может происходить как на стороне сервера, так и на стороне
клиента. Например -- не стоит нескольким обработчикам использовать один и тот же коннект
к БД. Менее очевидный пример: плохо, если разные обработчики будут читать записи с одних
и тех же страниц БД. Например, когда два обработчика читают чётные и нечётные записи
соответственно -- совсем не эффективно. Синхронизация на клиенте может возникнуть при
раздаче заданий, при обработке полученных данных (при выделение памяти под результаты) и так далее.

Для "честного" разбиения одна из проблем в том, что клиенту не известно как именно
распределены записи по страницам (и по ключам индексов), сколько вообще есть записей (для
больших таблиц дорого считать заранее), да и сколько есть страниц -- тоже дорого посчитать.
Серверу это обычно тоже не известно.

Ниже описано как это делает `gbak`.

В `gbak` единицей работы является набор записей со страниц данных (DP), принадлежащих одной
и той же странице указателей (pointer page, или PP). Это, с одной стороны, достаточно большое
количество записей, чтобы обеспечить обработчик работой без необходимости часто просить новый кусок данных
(синхронизация). С другой стороны, даже если такие наборы записей будут иметь не очень
одинаковый размер, их (наборов) количество позволит относительно равномерно загрузить работой все
обработчики. То есть вполне возможны случаи, когда один обработчик прочитает N записей с одной PP, а
другой -- M записей, и M будет достаточно отличаться от N. Но это не проблема. Такой подход не идеален,
но он весьма прост в реализации и обычно достаточно эффективен, по крайней мере на больших
таблицах -- с десятками или сотнями (и больше) PP.

Теперь необходимо получить количество PP (Pointer Pages) для заданной таблицы. Это довольно легко,
а главное быстро, можно вычислить из таблицы `RDB$PAGES`:

[source,sql]
====
SELECT RDB$PAGE_SEQUENCE 
FROM RDB$PAGES
WHERE RDB$RELATION_ID = ? AND RDB$PAGE_TYPE = 4
ORDER BY RDB$PAGE_SEQUENCE DESC ROWS 1
====

Далее можно было бы просто поделить количество PP на количество обработчиков, и выдать каждому свой
кусок. Но, как я писал выше, нет никакой гарантии, что такие "большие" куски будут означать
одинаковый объём работы. Нам же не интересно наблюдать как 15 обработчиков закончили свою работу
и простаивают, а 16-ый долго читает свои 100500 записей.

Поэтому в `gbak` это сделано иначе. Там есть координатор работы, который выдаёт каждому обработчику
по 1-ой PP за раз. Координатор знает сколько PP есть всего и сколько уже выдано в работу. Когда
обработчик прочитает свои записи, он обращается к координатору за новым номером PP. Это продолжается
до тех пор, пока не закончатся PP и пока есть работающие обработчики. Конечно, такое взаимодействие
обработчиков с координатором требует синхронизации. Опыт показывает, что объём работы, заданный
одной PP, позволяет не синхронизироваться слишком часто. Такой подход позволяет достаточно равномерно
загрузить работой все обработчики (а значит и ядра CPU) независимо от реального количества записей,
принадлежащих каждой PP.

Как же обработчик читает записи со своей PP? Для этого начиная с Firebird 4.0 (впервые появилось в HQBird 2.5) есть новая
встроенная функция `MAKE_DBKEY()`. С её помощью можно получить `RDB$DB_KEY` (физический номер записи) для первой записи на указанной PP. 
И с помощью таких `RDB$DB_KEY` и осуществляется отбор нужных записей:

[source,sql]
====
SELECT * 
FROM relation
WHERE RDB$DB_KEY >= MAKE_DBKEY(:rel_id, 0, 0, :loPP)
    AND RDB$DB_KEY < MAKE_DBKEY(:rel_id, 0, 0, :hiPP)
====

Теперь, когда есть представление о том как работает `gbak` можно перейти к описанию реализации утилиты `CSVExport`.

== Реализация утилиты `CSVExport`
