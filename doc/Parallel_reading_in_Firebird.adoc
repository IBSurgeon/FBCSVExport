[[parallel-reading]]
= Параллельное чтение данных в СУБД Firebird
Симонов Денис, Влад Хорсун
v1.0 от 03.12.2023
:doctype: book
:sectnums!:
:sectanchors:
:experimental:
:lang: ru
:imagesdir: images
ifdef::backend-pdf[]
:pdf-fontsdir: theme/fonts
:pdf-themesdir: theme/firebird-pdf
:pdf-theme: firebird
:source-highlighter: coderay
endif::[]
ifdef::backend-html5[]
:stylesdir: theme/firebird-html
:stylesheet: firebird.css
:source-highlighter: highlight.js
endif::[]

[dedication%notitle]
--
Этот материал был создан при поддержке и спонсорстве компании https://www.ibase.ru[iBase.ru], которая разрабатывает  инструменты Firebird SQL для предприятий и предоставляет сервис технической поддержки для Firebird SQL.

Материал выпущен под лицензией Public Documentation License https://www.firebirdsql.org/file/documentation/html/en/licenses/pdl/public-documentation-license.html
--

[preface]
== Предисловие

В Firebird 5.0 появилась возможность использовать параллелизм при создании резервной копии с помощью утилиты `gbak`. 
Надо отметить, что первоначально эта функция появилась в HQbird 2.5, и уже после обкатке её реальными заказчиками была портирована в Firebird 5.0.

В этой статье мы познакомимся с некоторыми механизмами, которые используется при параллельном создании резервных копий внутри утилиты `gbak`,
а также покажем как их можно использовать в ваших приложениях для параллельного чтения данных.

Важно отметить, что здесь идёт речь не о параллельном сканировании таблиц внутри движка Firebird при выполнении SQL запросов, а чтении данных внутри вашего приложения
параллельными потоками.

Для демонстрации параллельного чтения данных из СУБД Firebird я написал простенькую утилиту, которые экспортирует данные из одной или нескольких таблиц в формат CSV.
Её описание и сходный код можно найти по адресу https://github.com/IBSurgeon/FBCSVExport.git

== Организация параллельного чтения

И так, давайте подумаем как читать данные из нескольких таблиц параллельно. Для начала надо отметить, что Firebird, позволяет выполнять запросы параллельно только если каждый из запросов будет выполняться в отдельном соединении. 

Для этой цели необходимо сделать пул рабочих потоков. Основной поток приложения
также является рабочим потоком, поэтому количество дополнительных рабочих потоков должно быть равно N - 1, где N - количество параллельных обработчиков. Каждый рабочий поток будет работать с собственным соединением и транзакцией.

И тут возникает первая проблема, а как обеспечить согласованность прочитанных данных? 

=== Согласованное чтение данных

Поскольку каждый рабочий поток использует собственное соединение и собственную транзакцию, то возникает проблема несогласованного чтения - если таблица параллельно меняется, то прочитанные данные могут быть несогласованными. В однопоточном режиме `gbak` использовал транзакцию с режимом изолированности SNAPSHOT, что позволяло читать согласованную информацию, на начало старта SNAPSHOT транзакции. Но здесь у нас несколько транзакций и необходимо, чтобы они видели один и тот же "снимок", для того чтобы они читали одни и те же неизменные данные.

Механизм создания общего снимка для разных транзакций с режимом изолированности SNAPSHOT появился в Firebird 4.0 (первоначально HQBird 2.5, но в Firebird 4.0/HQBird 4.0 он более простая и эффективная). Для создания общего снимка существует два способа:

1. Через SQL.
  - получаем номер моментального снимка из главной транзакции, то есть транзакции с которой работает основной рабочий поток.
+  
[source,sql]
----
SELECT RDB$GET_CONTEXT('SYSTEM', 'SNAPSHOT_NUMBER') FROM RDB$DATABASE
----
  
  - стартуем другие транзакции запросом:
+
[source,sql]
----
SET TRANSACTION SNAPSHOT AT NUMBER snapshot_number
----
+
   где `snapshot_number` номер моментального снимка.

2. Через API.
  - получить номер моментального снимка из главной транзакции, то есть транзакции с которой работает основной рабочий поток, используя функцию
   `isc_transaction_info` или `ITransaction.getInfo` с тегом `fb_info_tra_snapshot_number`;
  - стартуем другие транзакции, передавая туда номер снимка с помощью тега `isc_tpb_at_snapshot_number`.
  
`CSVExport` так же как и `gbak` использует второй подход. Эти подходы можно комбинировать, например получив номер моментального снимка запросом, но передавая этот номер
для старта других транзакций с помощью API или наоборот.

В `CSVExport` получение номера моментального снимка реализовано следующим образом:

[source,cpp]
----
ISC_INT64 getSnapshotNumber(Firebird::ThrowStatusWrapper* status, Firebird::ITransaction* tra)
{
    ISC_INT64 ret = 0;
    unsigned char in_buf[] = { fb_info_tra_snapshot_number, isc_info_end };
    unsigned char out_buf[16] = { 0 };

    tra->getInfo(status, sizeof(in_buf), in_buf, sizeof(out_buf), out_buf);

    unsigned char* p = out_buf, * e = out_buf + sizeof(out_buf);
    while (p < e)
    {
        short len = 0;
        switch (*p++)
        {
        case isc_info_error:
        case isc_info_end:
            p = e;
            break;

        case fb_info_tra_snapshot_number:
            len = static_cast<short>(isc_vax_integer(reinterpret_cast<char*>(p), 2));
            p += 2;
            ret = isc_portable_integer(p, len);
            p += len;
            break;
        }
    }
    return ret;
}
----  

А старт транзакции с полученным номером моментального снимка так:

[source,sql]
----
Firebird::AutoDispose<Firebird::IXpbBuilder> tpbWorkerBuilder(fbUtil->getXpbBuilder(&status, Firebird::IXpbBuilder::TPB, nullptr, 0));
tpbWorkerBuilder->insertTag(&status, isc_tpb_concurrency);
tpbWorkerBuilder->insertBigInt(&status, isc_tpb_at_snapshot_number, snapshotNumber);

Firebird::AutoRelease<Firebird::ITransaction> workerTra(
    workerAtt->startTransaction(
        &status,
        tpbWorkerBuilder->getBufferLength(&status),
        tpbWorkerBuilder->getBuffer(&status)
    )
);
----

Теперь данные прочитанные из разных соединений будут согласованными, можно
распределять нагрузку по рабочим потокам.

Как именно распределить нагрузку между рабочими потоками? В случае полного экспорта таблиц или резервной копии самым простым вариантом
является - один рабочий поток на одну таблицу. Но при таком подходе есть одна проблема: а что если у нас много маленьких и одна большая таблица или вообще таблица всего одна и она огромная?
В этом случае какому-то потоку достанется большая таблица, а остальные потоки будут простаивать. Для того чтобы этого не происходило необходимо обрабатывать большую таблицу по частям.

[NOTE]
====
Дальнейшие рассуждения касаются только полного чтения таблиц, если вы хотите организовать параллельное чтение из некоторого запроса (представления),
то вам необходимо продумать как разбивать этот запрос на части самостоятельно. 
====

=== Разбиение большой таблицы на части

Допустим у нас всего одна большая таблица, которую хотим прочитать целиком и как можно быстрее.
Предлагается разбить её на несколько частей и каждую часть читать из своего
потока независимо. Каждый поток должен иметь свой коннект с БД.

В этом случае возникают следующие вопросы:

- на сколько частей разбить таблицу?
- как это лучше сделать?

Ответим на эти вопросы по порядку.

==== На сколько частей разбить таблицу

Для начала предположим идеальный вариант -- сервер и клиент больше ничем не заняты,
то есть все CPU полностью в нашем распоряжении. Тогда я бы рекомендовал:

а) взять за максимальное количество частей двойное количество ядер на сервере.
Обычно рекомендуется разбивать подобные параллельные задачи согласно количеству ядер,
но мы точно знаем, что у нас будут задержки связанные с IO, поэтому можем себе
позволить некоторое превышение. Более точно определить может только практика.

б) учитывать количество ядер на клиенте: если на сервере их сильно больше (обычная ситуация),
то возможно будет иметь смысл сильнее ограничить количество частей разбиения, чтобы не
перегружать клиента (он всё равно больше не сможет обработать, а расходы на переключение
потоков никуда не денутся). Точнее можно будет решить, наблюдая за загрузкой CPU клиента и
сервера -- если на клиенте 100%, а на сервере заметно меньше, то имеет смысл уменьшить
количество частей.

в) если клиент и сервер -- один и тот же хост, то см. (а)

Если клиент и/или сервер заняты чем-то ещё, то возможно придётся ещё уменьшить
количество частей. Так же на это может повлиять способность дисков на сервере обрабатывать
множество IO запросов одновременно (наблюдать за размером очереди и временем отклика).

==== Как лучше разбить таблицу на части

Для любой параллельной обработки важно обеспечить равномерное распределение заданий по
обработчикам и свести к минимуму их взаимную синхронизацию. Причём нужно помнить, что
синхронизация обработчиков может происходить как на стороне сервера, так и на стороне
клиента. Например -- не стоит нескольким обработчикам использовать один и тот же коннект
к БД. Менее очевидный пример: плохо, если разные обработчики будут читать записи с одних
и тех же страниц БД. Например, когда два обработчика читают чётные и нечётные записи
соответственно -- совсем не эффективно. Синхронизация на клиенте может возникнуть при
раздаче заданий, при обработке полученных данных (при выделение памяти под результаты) и так далее.

Для "честного" разбиения одна из проблем в том, что клиенту не известно как именно
распределены записи по страницам (и по ключам индексов), сколько вообще есть записей (для
больших таблиц дорого считать заранее), да и сколько есть страниц -- тоже дорого посчитать.
Серверу это обычно тоже не известно.

Ниже описано как это делает `gbak`.

В `gbak` единицей работы является набор записей со страниц данных (DP), принадлежащих одной
и той же странице указателей (pointer page, или PP). Это, с одной стороны, достаточно большое
количество записей, чтобы обеспечить обработчик работой без необходимости часто просить новый кусок данных
(синхронизация). С другой стороны, даже если такие наборы записей будут иметь не очень
одинаковый размер, их (наборов) количество позволит относительно равномерно загрузить работой все
обработчики. То есть вполне возможны случаи, когда один обработчик прочитает N записей с одной PP, а
другой -- M записей, и M будет достаточно отличаться от N. Но это не проблема. Такой подход не идеален,
но он весьма прост в реализации и обычно достаточно эффективен, по крайней мере на больших
таблицах -- с десятками или сотнями (и больше) PP.

Теперь необходимо получить количество PP (Pointer Pages) для заданной таблицы. Это довольно легко,
а главное быстро, можно вычислить из таблицы `RDB$PAGES`:

[source,sql]
====
SELECT RDB$PAGE_SEQUENCE 
FROM RDB$PAGES
WHERE RDB$RELATION_ID = ? AND RDB$PAGE_TYPE = 4
ORDER BY RDB$PAGE_SEQUENCE DESC ROWS 1
====

Далее можно было бы просто поделить количество PP на количество обработчиков, и выдать каждому свой
кусок. Но, как я писал выше, нет никакой гарантии, что такие "большие" куски будут означать
одинаковый объём работы. Нам же не интересно наблюдать как 15 обработчиков закончили свою работу
и простаивают, а 16-ый долго читает свои 100500 записей.

Поэтому в `gbak` это сделано иначе. Там есть координатор работы, который выдаёт каждому обработчику
по 1-ой PP за раз. Координатор знает сколько PP есть всего и сколько уже выдано в работу. Когда
обработчик прочитает свои записи, он обращается к координатору за новым номером PP. Это продолжается
до тех пор, пока не закончатся PP и пока есть работающие обработчики. Конечно, такое взаимодействие
обработчиков с координатором требует синхронизации. Опыт показывает, что объём работы, заданный
одной PP, позволяет не синхронизироваться слишком часто. Такой подход позволяет достаточно равномерно
загрузить работой все обработчики (а значит и ядра CPU) независимо от реального количества записей,
принадлежащих каждой PP.

Как же обработчик читает записи со своей PP? Для этого начиная с Firebird 4.0 (впервые появилось в HQBird 2.5) есть
встроенная функция `MAKE_DBKEY()`. С её помощью можно получить `RDB$DB_KEY` (физический номер записи) для первой записи на указанной PP. 
И с помощью таких `RDB$DB_KEY` и осуществляется отбор нужных записей:

[source,sql]
====
SELECT * 
FROM relation
WHERE RDB$DB_KEY >= MAKE_DBKEY(:rel_id, 0, 0, :loPP)
    AND RDB$DB_KEY < MAKE_DBKEY(:rel_id, 0, 0, :hiPP)
====

Например, если задать loPP = 0 и hiPP = 1, то будут прочитаны все записи с PP = 0, и только из неё.

Теперь, когда есть представление о том как работает `gbak` можно перейти к описанию реализации утилиты `CSVExport`.

== Реализация утилиты `CSVExport`

Утилита `CSVExport` предназначена для экспорта данных из таблиц БД Firebird в формат CSV.

Каждая таблица экспортируется в файл с именем `<tablename>.csv`. В обычном (однопоточном режиме)
данные из таблиц экспортируется последовательно в алфавитном порядке имени таблиц.

В параллельном режиме, таблицы экспортируются параллельно, каждая таблица в отдельном потоке. Если
таблица очень большая, то она разбивается на части, и каждая часть экспортируется в отдельном потоке.
Для каждой части большой таблицы создаётся отдельный файл с именем `<tablename>.csv.partN`, где N - номер части.
Когда все части большой таблицы экспортированы, файлы частей сливаются в общий файл с именем `<tablename>.csv`.

Для того, чтобы указать какие именно таблицы будут экспортированы используется регулярное выражение.
Возможен экспорт только обычных таблиц (системные таблицы, GTT, представления, внешние таблицы не поддерживаются).
Регулярные выражения должны быть в SQL синтаксисе, то есть такие, которые используются в предикате `SIMILAR TO`.

Для отбора списка экспортируемых таблиц, а также списка их PP в многопоточном режиме я использую следующий запрос:

[source,sql]
----
SELECT
    R.RDB$RELATION_ID AS RELATION_ID,
    TRIM(R.RDB$RELATION_NAME) AS RELATION_NAME,
    P.RDB$PAGE_SEQUENCE AS PAGE_SEQUENCE,
    COUNT(P.RDB$PAGE_SEQUENCE) OVER(PARTITION BY R.RDB$RELATION_NAME) AS PP_CNT
FROM RDB$RELATIONS R
JOIN RDB$PAGES P ON P.RDB$RELATION_ID = R.RDB$RELATION_ID
WHERE R.RDB$SYSTEM_FLAG = 0 AND
      R.RDB$RELATION_TYPE = 0 AND
      P.RDB$PAGE_TYPE = 4 AND
      TRIM(R.RDB$RELATION_NAME) SIMILAR TO CAST(? AS VARCHAR(8191))
ORDER BY R.RDB$RELATION_NAME, P.RDB$PAGE_SEQUENCE
----

В однопоточном режиме этот запрос можно упростить до

[source,sql]
----
SELECT
    R.RDB$RELATION_ID AS RELATION_ID,
    TRIM(R.RDB$RELATION_NAME) AS RELATION_NAME,
    0 AS PAGE_SEQUENCE,
    1 AS PP_CNT
FROM RDB$RELATIONS R
WHERE R.RDB$SYSTEM_FLAG = 0 AND
      R.RDB$RELATION_TYPE = 0 AND
      TRIM(R.RDB$RELATION_NAME) SIMILAR TO CAST(? AS VARCHAR(8191))
ORDER BY R.RDB$RELATION_NAME
----

В однопоточном режиме значения полей `PAGE_SEQUENCE` и `PP_CNT` не используются, они добавлены в запрос с целью унификации выходных сообщений.

Результат этого запроса складывается в вектор структур:

[source,cpp]
----
struct TableDesc
{
    TableDesc() = default;
    TableDesc(const OutputRecord& rec)
        : releation_id(rec->releation_id)
        , relation_name(rec->relation_name.str, rec->relation_name.length)
        , page_sequence(rec->page_sequence)
        , pp_cnt(rec->pp_cnt)
    {}

    short releation_id;
    std::string relation_name;
    int32_t page_sequence;
    int64_t pp_cnt;
};
----

Этот вектор заполняется при помощи функции объявленной как:

[source,cpp]
----
std::vector<TableDesc> getTablesDesc(
    Firebird::ThrowStatusWrapper* status,
    Firebird::IAttachment* att,
    Firebird::ITransaction* tra,
    unsigned int sqlDialect,
    const std::string& tableIncludeFilter,
    bool singleWorker = true);
----

Последний параметр `singleWorker` переключает режим заполнения `std::vector<TableDesc>`, если
`singleWorker = true`, то используется запрос для однопоточного режима, если `singleWorker = false`, то
используется более дорогой и сложный запрос для многопоточного режима. Саму реализацию я не буду приводить,
она довольно проста, и вы можете посмотреть её в исходном коде проекта.

Для экспорта таблицы в формат CSV разработан класс `CSVExportTable`, который содержит следующие методы:

[source,cpp]
----
    void prepare(Firebird::ThrowStatusWrapper* status, const std::string& tableName, 
                 unsigned int sqlDialect, bool withDbkeyFilter = false);

    void printHeader(Firebird::ThrowStatusWrapper* status, csv::CSVFile& csv);

    void printData(Firebird::ThrowStatusWrapper* status, csv::CSVFile& csv, int64_t ppNum = 0);
----

Метод `prepare` предназначен для построения и подготовки запроса, который используется для экспорта таблицы
в формат CSV. Внутренний запрос строится по разному в зависимости от параметра `withDbkeyFilter`.
Если `withDbkeyFilter = true`, то запрос строится с фильтрацией по диапазону `RDB$DB_KEY`:

[source,sql]
----
SELECT *
FROM tableName
WHERE RDB$DB_KEY >= MAKE_DBKEY('tableName', 0, 0, ?)
  AND RDB$DB_KEY < MAKE_DBKEY('tableName', 0, 0, ?)
----

в противном случае используется упрощённый запрос:

[source,sql]
----
SELECT *
FROM tableName
----

Значение параметра `withDbkeyFilter` устанавливается в `true`, если используется многопоточный режим, и таблица является большой.
Считаем таблицу большой, если `pp_cnt > 1`.

Метод `printHeader` предназначен для печати заголовка CSV файла (имён столбцов таблицы).

Метод `printData` печатает данные таблицы в CSV файл c PP страницы с номером `ppNum`, если запрос был подготовлен с использованием фильтра
по диапазону `RDB$DB_KEY`, и всех данных таблицы в противном случае.

Теперь посмотрим фрагмент кода для работы в однопоточном режиме

[source,cpp]
----
...

// Открываем главное соединение
Firebird::AutoRelease<Firebird::IAttachment> att(
    provider->attachDatabase(
        &status,
        m_database.c_str(),
        dbpLength,
        dpb
    )
);

// Стартуем главную транзакцию в режиме изолированности SNAPSHOT
Firebird::AutoDispose<Firebird::IXpbBuilder> tpbBuilder(fbUtil->getXpbBuilder(&status, Firebird::IXpbBuilder::TPB, nullptr, 0));
tpbBuilder->insertTag(&status, isc_tpb_concurrency);

Firebird::AutoRelease<Firebird::ITransaction> tra(
    att->startTransaction(
        &status,
        tpbBuilder->getBufferLength(&status),
        tpbBuilder->getBuffer(&status)
    )
);
// Получаем список таблиц по регулярному выражению в m_filter.
// m_parallel задаёт количество параллельных потоков, когда она равна 1,
// то используется упрощённый запрос для получения списка таблиц,
// в противном случае, для каждой таблицы формируется список PP и их количество.
auto tables = getTablesDesc(&status, att, tra, m_sqlDialect, m_filter, m_parallel == 1);

if (m_parallel == 1) {
    FBExport::CSVExportTable csvExport(att, tra, fb_master);
    for (const auto& tableDesc : tables) {
        // здесь нет смысла использовать фильтр по диапазону RDB$DB_KEY
        csvExport.prepare(&status, tableDesc.relation_name, m_sqlDialect, false);
        const std::string fileName = tableDesc.relation_name + ".csv";
        csv::CSVFile csv(m_outputDir / fileName);
        if (m_printHeader) {
            csvExport.printHeader(&status, csv);
        }
        csvExport.printData(&status, csv);
    }
}
----

Здесь всё довольно просто и не требует дополнительных пояснений, поэтому перейдем к многопоточной части.

Для того, чтобы экспорт происходил в многопоточном режиме, необходимо создать дополнительные `m_parallel - 1` рабочих потоков.
Почему количество дополнительных потоков на 1 меньше? Да потому что основной поток, тоже будет заниматься экспортом данных и он
является равноправным с дополнительными потоками. Вынесем общую часть основного и дополнительного потока в отдельную функцию:

[source,cpp]
----
void ExportApp::exportByTableDesc(Firebird::ThrowStatusWrapper* status, FBExport::CSVExportTable& csvExport, const TableDesc& tableDesc)
{
    // Если в tableDesc pp_cnt > 1, то она описывает только часть таблицы, и необходимо построить
    // запрос с использованием фильтра по диапазону RDB$DB_KEY. 
    bool withDbKeyFilter = tableDesc.pp_cnt > 1;
    csvExport.prepare(status, tableDesc.relation_name, m_sqlDialect, withDbKeyFilter);
    std::string fileName = tableDesc.relation_name + ".csv";
    // Если это не первая часть таблицы, то записываем эту часть в файл <tableName>.csv.part<N>, где
    // N - номер PP. Позднее части таблицы будут соединены в единый файл <tableName>.csv
    if (tableDesc.page_sequence > 0) {
        fileName += ".part_" + std::to_string(tableDesc.page_sequence);
    }
    csv::CSVFile csv(m_outputDir / fileName);
    // Заголовок CSV файла нужно печатать только в первую часть таблицы.
    if (tableDesc.page_sequence == 0 && m_printHeader) {
        csvExport.printHeader(status, csv);
    }
    csvExport.printData(status, csv, tableDesc.page_sequence);
}
----
